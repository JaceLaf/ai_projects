{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Intelligence 601.464\n",
    "### Project #2\n",
    "\n",
    "### Before You Begin...\n",
    "00. We're using a Jupyter Notebook environment (tutorial available here: https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html),\n",
    "01. Read the entire notebook before beginning your work, and\n",
    "02.  Check the submission deadline on Gradescope.\n",
    "\n",
    "\n",
    "### General Directions for this Assignment\n",
    "00. Output format should be exactly as requested,\n",
    "01. Functions should do only one thing,\n",
    "02. Keep functions to 20 lines or less (empty lines are fine, there's leeway, but don't blatantly ignore this),\n",
    "03. Add docstring to all functions,\n",
    "\n",
    "\n",
    "### Before You Submit...\n",
    "00. Re-read the general instructions provided above, and\n",
    "01. Hit \"Kernel\"->\"Restart & Run All\". The first cell that is run should show [1], the second should show [2], and so on...\n",
    "02. Submit your notebook (as .ipynb, not PDF) using Gradescope, and\n",
    "03.  Do not submit any other files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem #0: Conda and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using non-standard libaries for this assignment. We will be using **gymnansium** a python library for Reinforcement Learning simulations (we will not need RL for this project), **numpy** for fast vector math, **matplotlib** for graphing, and **pillow** for rendering. \n",
    "\n",
    "***Installing Imports***\n",
    "\n",
    "To ensure consistency with outputs, we recommend using a python environment to install these libraries. We recommend using conda. Miniconda is sufficient: https://www.anaconda.com/docs/getting-started/miniconda/main\n",
    "\n",
    "Once conda is installed, create your own environment by running:\n",
    "\n",
    "```conda create -n \"ai_hw2\" python==3.13.9```\n",
    "\n",
    "This ensures your python version aligns with other machines (our autograder) and has a clean install of python.\n",
    "To install your libraries:\n",
    "\n",
    "```\n",
    "conda activate \"ai_hw2\"\n",
    "pip install gymnasium gymnasium[other] ipykernel matplotlib\n",
    "```\n",
    "--By default gymnasium should require numpy and pillow, so we can omit that in our install.\n",
    "\n",
    "Now we select our \"ai_hw2\" kernel when running our code and we have a consistent python run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem #1: Boids Rule Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from PIL import Image, ImageDraw\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from IPython.display import Video\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Aside: Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`numpy` is a library used for fast vector math! Instead of using lists which can be slow and memory intensive we define arrays using \n",
    "\n",
    "`x = np.array([1, 2, 3])`\n",
    "\n",
    "- x is now an array of [1,2,3]! \n",
    "- We can also consider this a vector (x,y,z)\n",
    "- Arrays are faster since they are contiguous in memory!\n",
    "\n",
    "**Defining Matrices**\n",
    "\n",
    "We can now create efficient matrices by doing the following:\n",
    "\n",
    "- `x = np.zeros(2)` -- creates an 1x2 matrix of zeros\n",
    "- `y = np.ones((3,3))` -- creates a 3x3, matrix of ones\n",
    "- `z = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])` -- creates a 3x3 matrix of predefined values.\n",
    "\n",
    "**Vector Math**\n",
    "\n",
    "- We can add matrices together by using the `+` operator. \n",
    "- We can multiply them elementwise by using the `*` operator. You can also scale matrices with a scalar using the same operator!\n",
    "- We can mutiply matrices by using the `@` operator (not necessary yet, but helpful to know!)\n",
    "- Numpy supports linear algera functions like norm, inverse, and transpose!\n",
    "\n",
    "**Numpy dtypes**\n",
    "- Since arrays are contigous in memory, arrays must be of one type. We cannot have floats and ints in the same array...\n",
    "- By default numpy assigns arrays to be `np.float32` which is a float with 32 bits. This is ideal for fast physics calculations that require precision and speed. \n",
    "    - Other common types are `np.int32` and `np.long` for integer values and `np.float16` and `np.float64` for more optimized or fine-grained tasks.\n",
    "- We can define our array types by passing an additional argument \n",
    "\n",
    "    - `y = np.ones((3,3), dtype=np.float32)` -- 3x3 matrix of floats\n",
    "\n",
    "- When using math oprators arrays can cast when needed, but it is good practice to keep your array types consistent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros(2)\n",
    "y = np.ones((3,3), dtype=np.float32)\n",
    "z = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(\"Addition\\n\", y+z)\n",
    "print(\"Multiplication\\n\", y*z)\n",
    "print(\"Matrix multiplication\\n\", y@z)\n",
    "print(\"Scalar multiplication\\n\", z*2)\n",
    "# print(x+y) NOT POSSIBLE, DIMENSION MISMATCH\n",
    "k = np.ones((3,3), dtype=np.int32)\n",
    "print(\"Addition with int32\\n\", k+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Vector helper functions \n",
    "\n",
    "Vector math for:\n",
    "\n",
    "- Normalizing direction vectors (unit)\n",
    "- Limiting acceleration magnitude (clip_vec)\n",
    "= Combining multiple “forces” into a single acceleration (combine_accels)\n",
    "\n",
    "You **do not** need to modify these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def safe_norm(v: np.ndarray, eps=1e-8) -> float:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        v: vector to compute norm of\n",
    "        eps: small value to add for numerical stability\n",
    "    Returns:\n",
    "        ||v|| with epsilon to avoid divide-by-zero.\n",
    "    \"\"\"\n",
    "    return float(np.linalg.norm(v) + eps)\n",
    "\n",
    "def unit(v: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        v: vector to compute unit vector of\n",
    "    Returns:\n",
    "        Return unit vector in direction of v (or zeros if v is ~0).\n",
    "    \"\"\"\n",
    "    n = safe_norm(v)\n",
    "    if n <= 1e-7:\n",
    "        return np.zeros_like(v, dtype=np.float32)\n",
    "    return (v / n).astype(np.float32)\n",
    "\n",
    "def clip_vec(v: np.ndarray, max_mag: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Args:        \n",
    "        v: vector to clip\n",
    "        max_mag: maximum magnitude\n",
    "    Returns:\n",
    "        Clip vector magnitude to max_mag (direction preserved).\n",
    "    \"\"\"\n",
    "    n = safe_norm(v)\n",
    "    if n <= max_mag:\n",
    "        return v.astype(np.float32)\n",
    "    return (v * (max_mag / n)).astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Boids helper functions \n",
    "\n",
    "- Repulsion: move away from nearby swarm neighbors\n",
    "- Attraction: move toward the leader\n",
    "- Obstacle avoidance: push away if near/inside obstacle region\n",
    "These functions convert Boids ideas into acceleration vectors.\n",
    "\n",
    "<span style=\"color: red;\"> Modify `combine_acccels`, `repulsion_accel` and `attraction_accel`.\n",
    "You will use them in the TODOs later!</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_accels(accels:list[np.ndarray], amax:float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        accels: list of acceleration vectors to combine\n",
    "        amax: maximum allowed acceleration magnitude\n",
    "    Returns:\n",
    "        Sum a list of acceleration vectors and clip to amax.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def repulsion_accel(my_pos: np.ndarray, neighbor_positions: np.ndarray, radius: float, gain: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Repulsion: sum of push-away vectors from neighbors within 'radius'.\n",
    "    Args:\n",
    "        my_pos: my position\n",
    "        neighbor_positions: array of neighbor positions\n",
    "        radius: distance within which to repel neighbors\n",
    "        gain: strength of repulsion\n",
    "    Returns:\n",
    "        Returns an acceleration vector.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def attraction_accel(my_pos: np.ndarray, leader_pos: np.ndarray, gain: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Attraction: pull toward the leader.\n",
    "    Args:\n",
    "        my_pos: agent position\n",
    "        leader_pos: leader position\n",
    "        gain: strength of attraction\n",
    "    Returns:\n",
    "        Returns an acceleration vector.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def obstacle_avoid_accel(my_pos: np.ndarray, rect: tuple, avoid_dist: float, gain: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Obstacle avoidance for axis-aligned rectangle rect=(x0,y0,x1,y1).\n",
    "    If within avoid_dist of the rectangle (including inside), push away.\n",
    "    Args:\n",
    "        my_pos: agent position\n",
    "        rect: rectangle defined as (x0, y0, x1, y1)\n",
    "        avoid_dist: distance within which to avoid the rectangle\n",
    "        gain: strength of avoidance\n",
    "    Returns:\n",
    "        Returns an acceleration vector.\n",
    "    \"\"\"\n",
    "    x0, y0, x1, y1 = rect\n",
    "    closest = np.array([np.clip(my_pos[0], x0, x1),\n",
    "                        np.clip(my_pos[1], y0, y1)], dtype=np.float32)\n",
    "    away = (my_pos - closest).astype(np.float32)\n",
    "    d = safe_norm(away)\n",
    "\n",
    "    if d <= 1e-6:\n",
    "        left, right = my_pos[0] - x0, x1 - my_pos[0]\n",
    "        top, bottom = my_pos[1] - y0, y1 - my_pos[1]\n",
    "        m = min(left, right, top, bottom)\n",
    "        if m == left:\n",
    "            away = np.array([-1.0, 0.0], dtype=np.float32)\n",
    "        elif m == right:\n",
    "            away = np.array([1.0, 0.0], dtype=np.float32)\n",
    "        elif m == top:\n",
    "            away = np.array([0.0, -1.0], dtype=np.float32)\n",
    "        else:\n",
    "            away = np.array([0.0, 1.0], dtype=np.float32)\n",
    "        d = safe_norm(away)\n",
    "\n",
    "    if d > avoid_dist:\n",
    "        return np.zeros(2, dtype=np.float32)\n",
    "    return (gain * unit(away)).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gymnasium Environment \n",
    "Gym environment is just a standardized simulator API used to visualize your robot and swarm:\n",
    "- reset() initializes positions/velocities\n",
    "- step(action) applies accelerations and updates physics\n",
    "- render() draws the current world\n",
    "\n",
    "<span style=\"color: red;\">You **do not** need to modify ENV</span>\n",
    "\n",
    "Gynasium Basics: **Action Space vs. Observation Space**\n",
    "\n",
    "Observation Space is our agents data relative to the world. In Reinforcement Learning, this is what our agent is allowed to see. We do not modify manually, we only read this one. Here we observe each agents position (x,y) and velocity (vx, vy).\n",
    "\n",
    "$$\\text{Observation Space} = \\begin{bmatrix}\n",
    "                        x_1 & y_1 & v_{x1} & v_{y1} \\\\\n",
    "                        x_2 & y_2 & v_{x2} & v_{y2} \\\\\n",
    "                        x_3 & y_3 & v_{x3} & v_{y3} \\\\\n",
    "                        x_4 & y_4 & v_{x4} & v_{y4} \\\\\n",
    "                        x_5 & y_5 & v_{x5} & v_{y5} \\\\\n",
    "                    \\end{bmatrix}$$\n",
    "\n",
    "Action Space describes our agents next move. In Reinforcement Learning, this is what our agent is allowed to do. We modify this one in our policy. Here we modify each agents acceleration (ax, ay).\n",
    "$$\\text{Action Space} = \\begin{bmatrix}\n",
    "                    a_{x1} & a_{y1} \\\\\n",
    "                    a_{x2} & a_{y2} \\\\\n",
    "                    a_{x3} & a_{y3} \\\\\n",
    "                    a_{x4} & a_{y4} \\\\\n",
    "                    a_{x5} & a_{y5} \\\\\n",
    "                \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoidsEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    5 agents total:\n",
    "      idx 0 = leader\n",
    "      idx 1..4 = swarm\n",
    "\n",
    "    Observation: (5,4) array: [x, y, vx, vy]\n",
    "    Action:      (5,2) array: [ax, ay]\n",
    "    Reward: always 0 (ignored) (No RL here, just a simulation environment)\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": [\"rgb_array\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self, render_mode=\"rgb_array\", w=500, h=500, dt=1.0,\n",
    "                 vmax=6.0, amax=1.5, max_steps=300):\n",
    "        super().__init__()\n",
    "        self.render_mode = render_mode\n",
    "        self.w, self.h = int(w), int(h)\n",
    "        self.dt = float(dt)\n",
    "        self.vmax = float(vmax)\n",
    "        self.amax = float(amax)\n",
    "        self.max_steps = int(max_steps)\n",
    "\n",
    "        self.observation_space = spaces.Box(low=-1e9, high=1e9, shape=(5, 4), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=-self.amax, high=self.amax, shape=(5, 2), dtype=np.float32)\n",
    "\n",
    "        self.steps = 0\n",
    "        self.obstacles = [(150, 150, 350, 350)]\n",
    "        self.goal = np.array([450, 75], dtype=np.float32)\n",
    "        self.agents = np.zeros((5, 4), dtype=np.float32) # This will hold [x,y,vx,vy] for each agent\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.steps = 0\n",
    "        self.agents[:] = 0.0\n",
    "\n",
    "        # Initialize leader and swarm in a cluster near the bottom-left corner\n",
    "        self.agents[0, :2] = np.array([80, 450], dtype=np.float32)\n",
    "        for i in range(1, 5):\n",
    "            self.agents[i, :2] = np.array([80 + 20*i, 450], dtype=np.float32)\n",
    "\n",
    "        return self.agents.copy(), {}\n",
    "\n",
    "    def _clip_speed(self):\n",
    "        \"\"\"Clip each agent speed to vmax.\"\"\"\n",
    "        v = self.agents[:, 2:4]\n",
    "        speed = np.linalg.norm(v, axis=1, keepdims=True) + 1e-8\n",
    "        factor = np.minimum(1.0, self.vmax / speed)\n",
    "        self.agents[:, 2:4] = v * factor\n",
    "\n",
    "    def _bounce_walls(self):\n",
    "        \"\"\"Bounce off walls and keep agents inside [0,w]x[0,h].\"\"\"\n",
    "        x, y = self.agents[:, 0], self.agents[:, 1]\n",
    "        hitL, hitR = x < 0, x > self.w\n",
    "        hitT, hitB = y < 0, y > self.h\n",
    "\n",
    "        self.agents[hitL, 0] = 0\n",
    "        self.agents[hitR, 0] = self.w\n",
    "        self.agents[hitT, 1] = 0\n",
    "        self.agents[hitB, 1] = self.h\n",
    "\n",
    "        self.agents[hitL | hitR, 2] *= -1\n",
    "        self.agents[hitT | hitB, 3] *= -1\n",
    "\n",
    "    def _push_out_of_obstacles(self):\n",
    "        \"\"\"Physics correction so agents do not remain inside obstacle.\"\"\"\n",
    "        for rect in self.obstacles:\n",
    "            x0, y0, x1, y1 = rect\n",
    "            for i in range(5):\n",
    "                x, y = self.agents[i, 0], self.agents[i, 1]\n",
    "                if (x0 <= x <= x1) and (y0 <= y <= y1):\n",
    "                    left, right = x - x0, x1 - x\n",
    "                    top, bottom = y - y0, y1 - y\n",
    "                    m = min(left, right, top, bottom)\n",
    "                    if m == left:\n",
    "                        self.agents[i, 0] = x0 - 1\n",
    "                        self.agents[i, 2] *= -1\n",
    "                    elif m == right:\n",
    "                        self.agents[i, 0] = x1 + 1\n",
    "                        self.agents[i, 2] *= -1\n",
    "                    elif m == top:\n",
    "                        self.agents[i, 1] = y0 - 1\n",
    "                        self.agents[i, 3] *= -1\n",
    "                    else:\n",
    "                        self.agents[i, 1] = y1 + 1\n",
    "                        self.agents[i, 3] *= -1\n",
    "\n",
    "    def step(self, actions: np.ndarray):\n",
    "        \"\"\"\n",
    "        Apply accelerations, integrate physics, and handle boundaries.\n",
    "        Args:\n",
    "            actions: (5,2) array of accelerations for each agent\n",
    "        Returns:\n",
    "            obs: (5,4) array of agent states after step\n",
    "            reward: always 0.0 (No RL)\n",
    "            terminated: True if all agents are within 50 units of the goal\n",
    "            truncated: True if max_steps have been reached\n",
    "            info: empty dict\n",
    "        \"\"\"\n",
    "        # Convert actions to numpy array and clip to amax\n",
    "        actions = np.asarray(actions, dtype=np.float32)\n",
    "        actions = np.clip(actions, -self.amax, self.amax)\n",
    "\n",
    "        # Apply action to velocities and integrate positions\n",
    "        self.agents[:, 2:4] += actions * self.dt\n",
    "        self._clip_speed()\n",
    "        self.agents[:, 0:2] += self.agents[:, 2:4] * self.dt\n",
    "\n",
    "        # Handle boundaries and obstacles\n",
    "        self._bounce_walls()\n",
    "        self._push_out_of_obstacles()\n",
    "\n",
    "        # Check termination and truncation conditions\n",
    "        self.steps += 1\n",
    "        terminated = np.allclose(self.agents[:, :2], self.goal, atol=50)\n",
    "        truncated = (self.steps >= self.max_steps)\n",
    "\n",
    "        return self.agents.copy(), 0.0, terminated, truncated, {}\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Render an RGB frame using PIL\n",
    "        Returns an (H,W,3) uint8 array representing the RGB image.\n",
    "        \"\"\"\n",
    "        # Create white background image\n",
    "        img = Image.new(\"RGB\", (self.w, self.h), (255, 255, 255))\n",
    "        d = ImageDraw.Draw(img)\n",
    "\n",
    "        # Draw obstacles as gray rectangles\n",
    "        for (x0, y0, x1, y1) in self.obstacles:\n",
    "            d.rectangle([x0, y0, x1, y1], fill=(40, 40, 40))\n",
    "\n",
    "        # Draw goal as a green circle\n",
    "        gx, gy = float(self.goal[0]), float(self.goal[1])\n",
    "        d.ellipse([gx-50, gy-50, gx+50, gy+50], outline=(0, 200, 0), width=3)\n",
    "\n",
    "        # Draw agents (leader in blue, swarm in orange)\n",
    "        for i in range(5):\n",
    "            x, y = float(self.agents[i, 0]), float(self.agents[i, 1])\n",
    "            r = 10 if i == 0 else 8\n",
    "            color = (0, 120, 255) if i == 0 else (255, 140, 0)\n",
    "            d.ellipse([x-r, y-r, x+r, y+r], fill=color)\n",
    "\n",
    "        return np.array(img, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boids Policy\n",
    "\n",
    "At each timestep, every agent chooses an acceleration vector.\n",
    "The leader accelerates toward its next waypoint (path-following)\n",
    "Each swarm agent accelerates based on Boids forces:\n",
    "\n",
    "- **Repulsion from other swarm agents**\n",
    "- **Attraction to the leader**\n",
    "- **Obstacle avoidance**\n",
    "\n",
    "<span style=\"color: green;\">**Note: Leader _does not_ have repulsion, attraction or collision to swarm!**</span>\n",
    "\n",
    "This is going to determine how the butterflies decide to move! \n",
    "Complete the #TODOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoidsPolicy:\n",
    "    \"\"\"\n",
    "    Leader:\n",
    "      - follows waypoints\n",
    "      - obstacle avoidance\n",
    "\n",
    "    Swarm:\n",
    "      - repulsion from other swarm agents\n",
    "      - attraction to leader\n",
    "      - obstacle avoidance\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.wpt_idx = 0\n",
    "        self.waypoints = [(80, 100), (450, 75)]\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset internal waypoint index.\"\"\"\n",
    "        self.wpt_idx = 0\n",
    "\n",
    "    def leader_action(self, obs: np.ndarray, obstacle_rect: tuple, amax: float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Follow waypoints with obstacle avoidance.\n",
    "        Args:\n",
    "            obs: (5,4) array of agent states\n",
    "            obstacle_rect: rectangle to avoid (x0,y0,x1,y1)\n",
    "            amax: maximum acceleration magnitude\n",
    "        Returns:\n",
    "            Acceleration vector for the leader agent.\n",
    "        \"\"\"\n",
    "        # Get leader position\n",
    "        pos = obs[0, :2]\n",
    "\n",
    "        # Check if we are close to the current waypoint and switch to next\n",
    "        if self.wpt_idx < len(self.waypoints) - 1:\n",
    "            if np.allclose(pos, self.waypoints[self.wpt_idx], atol=18):\n",
    "                self.wpt_idx += 1\n",
    "        target = self.waypoints[self.wpt_idx]\n",
    "\n",
    "        # Simple proportional controller toward the target waypoint\n",
    "        a_goal = ((target - pos) * 0.05).astype(np.float32)\n",
    "\n",
    "        # Obstacle avoidance\n",
    "        a_avoid = obstacle_avoid_accel(pos, obstacle_rect, avoid_dist=40.0, gain=2.0)\n",
    "        return combine_accels([a_goal, a_avoid], amax)\n",
    "\n",
    "\n",
    "    def swarm_action(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        TODO (Swarm Boids rules):\n",
    "        0) Add func arguments\n",
    "        1) Read current and leader position\n",
    "        3) Build list of neighbor positions from other swarm agents\n",
    "        4) Repulsion:\n",
    "        5) Attraction to leader:\n",
    "        6) Obstacle avoidance:\n",
    "        7) Combine + clip:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def get_boids_actions(self, obs: np.ndarray, obstacle_rect: tuple, amax: float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        TODO :\n",
    "        1) Create actions\n",
    "        2) Set leader action\n",
    "        3) Set swarm actions\n",
    "        4) Return actions\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation\n",
    "\n",
    "Run this section of code to simulate the actions of your leader and swarm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(steps=300) -> bool: # AUTOGRADER DO NOT EDIT SIGNATURE\n",
    "    \"\"\"\n",
    "    Run the environment and render frames dynamically (NO matplotlib).\n",
    "    \"\"\"\n",
    "    # Create environment and policy\n",
    "    env = BoidsEnv()\n",
    "    policy = BoidsPolicy()\n",
    "    \n",
    "    # Record video to mp4 \n",
    "    video = RecordVideo(env, video_folder=\"./videos\", name_prefix=\"boids_eval\", disable_logger=True)\n",
    "\n",
    "    # Run simulation loop\n",
    "    obs, _ = video.reset()\n",
    "    policy.reset()\n",
    "    obstacle_rect = env.obstacles[0]\n",
    "    for _ in range(steps):\n",
    "        actions = policy.get_boids_actions(obs, obstacle_rect, amax=env.amax)\n",
    "        obs, _, terminated, truncated, _ = video.step(actions)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    # Cleanup and return video\n",
    "    video.close()\n",
    "    env.close()\n",
    "    return terminated\n",
    "\n",
    "# Run:\n",
    "run_simulation()\n",
    "video_files = [f for f in os.listdir(\"./videos\") if f.endswith(\".mp4\")]\n",
    "Video(f\"./videos/{video_files[-1]}\", embed=True, width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem #2: Game Theory\n",
    "\n",
    "We are going to conduct an experiment where two agents (players) employ the *Best Response with Inertia* algorithm against each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Response with Inertia Algorithm\n",
    "\n",
    "Here is the outline of the algorithm for each player:\n",
    "\n",
    "\n",
    "```\n",
    "On day zero, play a random action\n",
    "\n",
    "On day t, do one of two things:\n",
    "\n",
    "    * With high probability, play the best response to what your opponent did yesterday, or\n",
    "    * With low probability, be lazy and just do what you did yesterday.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game\n",
    "\n",
    "The particular 2x2 game being played is the Stag-Hunt game, with the matrix:\n",
    "\n",
    "\n",
    "|  | | |\n",
    "|---|---|---|\n",
    "|  | S | H |\n",
    "| S | 3, 3 | 0, 2 |\n",
    "| H | 2, 0 | 2, 2 |\n",
    "\n",
    "\n",
    "Player 1 (P1) is the `row` player; Player 2 (P2) is the `col` player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_utility\"></a>\n",
    "## get_utility\n",
    "\n",
    "This function takes a joint action tuple and returns the utility for each player, packaged as a tuple. It is hardcoded for the Stag-Hunt game described above.\n",
    "\n",
    "* **a** Tuple: joint action. a could be ('S', 'H'), for instance, which would signify that P1 has chosen to do 'S' and P2 has chosen to do 'H'.\n",
    "\n",
    "\n",
    "**returns** \n",
    "\n",
    "* **utility** Tuple: the utility for each player as (utility of P1, utility of P2). For example, if the input is ('S', 'H'), the returned value is (0, 2) -- 0 for P1, 2 for P2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_utility(a):\n",
    "    if a[0] == 'S' and a[1] == 'S':\n",
    "        return (3, 3)\n",
    "    elif a[0] == 'S' and a[1] == 'H':\n",
    "        return (0, 2)\n",
    "    elif a[0] == 'H' and a[1] == 'S':\n",
    "        return (2, 0)\n",
    "    elif a[0] == 'H' and a[1] == 'H':\n",
    "        return (2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: red; margin:20px; padding: 20px;\">\n",
    "    <p>\n",
    "        TODO: Help players pick actions based on the <i>Best Response with inertia</i> algorithm described above (instead of just picking 'S' like they are now). \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(num_trials, num_days, debug=True):\n",
    "    # The actions available to each player. Don't modify. \n",
    "    actions_P1 = ['S', 'H']\n",
    "    actions_P2 = ['S', 'H']\n",
    "    history_P1_action, history_P1_utility = [], []\n",
    "    history_P2_action, history_P2_utility = [], []\n",
    "    if debug: print(f\"Trial\\tDay\\tP1\\tP2\\n--------------------------\")\n",
    "    for trial in range(num_trials):  \n",
    "        if debug: print(f\"--------------------------\")\n",
    "        for day in range(num_days):\n",
    "\n",
    "            # ===\n",
    "            # FIX \n",
    "            # Player should pick an action using the Best Response\n",
    "            # with Inertia algorithm; instead of just picking 'S' all the time\n",
    "            # like they are doing now\n",
    "            # Use 0.75 as stand-in for \"high probability\"\n",
    "            # ===\n",
    "            action_P1 = 'S'\n",
    "            action_P2 = 'S'\n",
    "\n",
    "        \n",
    "            # Don't modify below:\n",
    "            utility_P1, utility_P2 = get_utility(tuple((action_P1, action_P2)))\n",
    "            history_P1_action.append(action_P1)\n",
    "            history_P2_action.append(action_P2)\n",
    "            history_P1_utility.append(utility_P1)\n",
    "            history_P2_utility.append(utility_P2)\n",
    "            if debug: print(f\"{trial}\\t{day}\\t{action_P1}\\t{action_P2}\")\n",
    "    return history_P1_action, history_P1_utility, history_P2_action, history_P2_utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters. Don't modify. Not even debug flag. \n",
    "result = run_experiment(num_trials=20, num_days=10, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(history_P1_action, history_P2_action):\n",
    "    labels = ['(S,S)', '(S,H)', '(H,S)', '(H,H)']\n",
    "    cell_counts = [0, 0, 0, 0]\n",
    "    for i in range(len(history_P1_action)):\n",
    "        if history_P1_action[i] == 'S' and history_P2_action[i] == 'S':\n",
    "            cell_counts[0]+=1\n",
    "        if history_P1_action[i] == 'S' and history_P2_action[i] == 'H':\n",
    "            cell_counts[1]+=1\n",
    "        if history_P1_action[i] == 'H' and history_P2_action[i] == 'S':\n",
    "            cell_counts[2]+=1\n",
    "        if history_P1_action[i] == 'H' and history_P2_action[i] == 'H':\n",
    "            cell_counts[3]+=1\n",
    "    plt.bar(labels, cell_counts)\n",
    "    plt.ylim(0, 200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results. Don't modify. \n",
    "plot_results(result[0], result[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem #3\n",
    "\n",
    "\n",
    "Revisit your response to Quiz #3, Question #5.3. Add your response below. If there's mismatch with Problem #2, clearly explain what you have learned in the process. If your results are consistent with your expectations, briefly explain the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Quiz #1, Question #5.3 | Briefly explain output OR clearly explain what you have learned | \n",
    "|------|------|\n",
    "|Problem #1 Output| N/A |\n",
    "|Problem #2 Output| YOUR COMMENTS HERE |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Submit...\n",
    "\n",
    "00. Re-read the general instructions provided above, and\n",
    "01. Hit \"Kernel\"->\"Restart & Run All\". The first cell that is run should show [1], the second should show [2], and so on...\n",
    "02. Submit your notebook (as .ipynb, not PDF) using Gradescope, and\n",
    "03.  Do not submit any other files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "192px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
